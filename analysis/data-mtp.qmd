---
title: "MTP"
format: html
engine: julia
---

### Load necessary packages
```{julia}
using DrWatson
quickactivate("analysis", "data-analysis")

using CSV, DataFrames
using CausalTables, Condensity, ModifiedTreatment
using MLJ
using Plots

using SimpleWeightedGraphs

using GLM
using Tables, TableTransforms
```

### Load data
```{julia}

# Load the dataframe
df_raw = CSV.read(joinpath("data","NO2_ZEV_ZCTAs.csv"), DataFrame)
df = select(sort(df_raw, :ZCTA), Not([:ZCTA, :n2_2019, :p__2013]))
df[!, "pop"] = float.(df_raw[!, "pop"]);

```

```{julia}
net_raw_2019 = CSV.read(joinpath("data","ZEV_commuters_2019.csv"), DataFrame)
net_raw_2013 = CSV.read(joinpath("data","ZEV_commuters_2013.csv"), DataFrame)

w = []
for net_raw in [net_raw_2019, net_raw_2013]
        net = filter(row -> row.h_zcta ∈ df_raw.ZCTA && row.w_zcta ∈ df_raw.ZCTA, net_raw)

        # create index mapping for adjacency matrix and replace IDs with rank
        zctas = sort(union(unique(net.w_zcta), unique(net.h_zcta)))
        zctas_dict = Dict(zctas .=> 1:length(zctas))
        net.w_zcta = map(x -> zctas_dict[x], net.w_zcta)
        net.h_zcta = map(x -> zctas_dict[x], net.h_zcta)

        # Construct graph and extract weight matrix
        g = SimpleWeightedDiGraph(net.w_zcta, net.h_zcta, net.weight)
        cur_w = g.weights
        #cur_w[g.weights .< 0.01] .= 0.0
        push!(w, cur_w)
end

neighbors = .!(iszero.(w[1]))


```

### Clean data
```{julia}

# Construct CausalTable with no summarization
ct_nosum = CausalTable(df; treatment = :ZEV_2019_pct, response = :no2)

# Construct CausalTable with network summaries
confounders = union(setdiff(Tables.columnnames(df), [:ZEV_2019_pct, :no2]), [:nfriends])
ct = CausalTable(df; treatment = :ZEV_2019_pct, response = :no2, 
                confounders = confounders,
                arrays = (F = neighbors, w_2019 = w[1], w_2013 = w[2],), 
                summaries = (ZEV_2019_sum = Sum(:ZEV_2019_pct, :w_2019),
                                ZEV_2013_sum = Sum(:ZEV_2013_pct, :w_2013),
                                nfriends = CausalTables.Friends(:F),
                                pop_s = CausalTables.Sum(:pop, :F),
                                medin_g_s = CausalTables.Sum(:medin_g, :F),
                                pct_cll_s = CausalTables.Sum(:pct_cll, :F),
                                pct_hgh_s = CausalTables.Sum(:pct_hgh, :F),
                                pct_wht_s = CausalTables.Sum(:pct_wht, :F),
                                mdn_ncm_s = CausalTables.Sum(:mdn_ncm, :F),
                                pct_pvr_s = CausalTables.Sum(:pct_pvr, :F),
                                pct_wn__s = CausalTables.Sum(:pct_wn_, :F),
                                mdn_hm__s = CausalTables.Sum(:mdn_hm_, :F),
                                pct_aut_s = CausalTables.Sum(:pct_aut, :F),
                                pct_pb_s = CausalTables.Sum(:pct_pb_, :F),
                                pct_wfh_s = CausalTables.Sum(:pct_wfh, :F),
                                D1C5_IN_s = CausalTables.Sum(:D1C5_IN, :F),
                                D3A_s = CausalTables.Sum(:D3A, :F),
                                D4C_s = CausalTables.Sum(:D4C, :F),
                                NtWlkIn_s = CausalTables.Sum(:NtWlkIn, :F)
                ))

```

## Linear regression

```{julia}
Xtbl = CausalTables.responseparents(ct_nosum)
X = Tables.matrix(Xtbl)
y = Tables.getcolumn(ct_nosum, ct_nosum.response[1])

glm_a = lm(X, y)

glm_effect_a = (est = coef(glm_a)[1], 
                   lower = confint(glm_a, 0.95)[1,1], 
                   upper = confint(glm_a, 0.95)[1,2])

y01 = (y .- minimum(y)) ./ (maximum(y) .- minimum(y))
glm_m = glm(X, y01, Binomial(), LogLink())

glm_effect_m = (est = coef(glm_m)[1], 
                   lower = confint(glm_m, 0.95)[1,1], 
                   upper = confint(glm_m, 0.95)[1,2])

```

## Baseline comparision function
We define this function to aid result aggregation. 
```{julia}
# Compute effect
ψ0 = mean(df.no2)
n = nrow(df)
function get_effects_tuple(mtp_result, ψ0, n)
        mtp_ψ = mtp_result.tmle.ψ .- ψ0
        ciwidth = 1.96 * sqrt(mtp_result.tmle.σ2net / n)
        (est = mtp_ψ, lower = mtp_ψ - ciwidth, upper = mtp_ψ + ciwidth)
end

```

## MTP

First, define the super learners.

```{julia}
XGBoostRegressor = @load XGBoostRegressor pkg=XGBoost
XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost

mean_estimator = SuperLearner([
    XGBoostRegressor(objective = "reg:squarederror", booster="gblinear"),
    XGBoostRegressor(objective = "reg:squarederror", num_round = 1, 
                    colsample_bynode = 0.8, eta = 1, max_depth = 6, num_parallel_tree = 100, subsample = 0.8, tree_method = "hist"),
    XGBoostRegressor(objective = "reg:squarederror", 
                    eta = 0.1, max_depth = 6),
    XGBoostRegressor(objective = "reg:squarederror", 
                    eta = 0.01, max_depth = 6),
    XGBoostRegressor(objective = "reg:squarederror", 
                    num_round = 500, eta = 0.01, max_depth = 3),
], CV(nfolds = 5))

sl = SuperLearner([
    XGBoostClassifier(objective = "binary:logistic", booster="gblinear"),
    XGBoostClassifier(objective = "binary:logistic", num_round = 1, 
                      colsample_bynode = 0.8, eta = 1, max_depth = 6, num_parallel_tree = 100, subsample = 0.8, tree_method = "hist"),
    XGBoostClassifier(objective = "binary:logistic",
                      num_round = 100, eta = 0.1, max_depth = 6, subsample = 1.0),
    XGBoostClassifier(objective = "binary:logistic",
                      num_round = 100, eta = 0.1, max_depth = 3, subsample = 0.5),
    XGBoostClassifier(objective = "binary:logistic",
                      num_round = 500, eta = 0.01, max_depth = 3, subsample = 0.5),
], CV(nfolds = 5))

density_ratio_estimator = DensityRatioClassifier(sl)

cv_splitter = CV(nfolds = 5)
mtp = MTP(mean_estimator, density_ratio_estimator, cv_splitter)
```

Then, we run the MTP with no summarized features (assuming SUTVA).

```{julia}
shift = AdditiveShift(1.0)
mach = machine(mtp, ct_nosum, shift) |> fit!
```

```{julia}
mtp_result_a = getestimate(ModifiedTreatment.estimate(mach, shift))
mtp_effect_a = get_effects_tuple(mtp_result_a, ψ0, n)

```

Next, we run the MTP on the summarized data.

```{julia}

mach_net_sum = machine(mtp, ct, shift) |> fit!
mtp_result_net_a_sum = getestimate(ModifiedTreatment.estimate(mach_net_sum, AdditiveShift(1.0)))
mtp_effect_net_a_sum = get_effects_tuple(mtp_result_net_a_sum, ψ0, n)

```

```{julia}
ests = [mtp_effect_net_a_sum, mtp_effect_a, glm_effect_a]

tbl = (method = ["Induced MTP (Sum)", "MTP (No Interference)", "GLM (No Interference)"], effects = map(x -> x.est, ests), upper = map(x -> x.upper, ests), lower = map(x -> x.lower, ests))
CSV.write(joinpath("results", "mtp_analysis_a.csv"), tbl)

tbl

```


Let's also compute an MTP grid and save the results to a .CSV file.

```{julia}
est_grid_a = []
as = 0.2:0.2:2.0
for a in as
        shift = AdditiveShift(a)
        mtp_result_a = getestimate(estimate(mach, shift))
        mtp_result_net_a_sum = getestimate(estimate(mach_net_sum, shift))

        push!(est_grid_a, merge((shift = a, name = "MTP (No Interference)"), get_effects_tuple(mtp_result_a, ψ0, n)))
        push!(est_grid_a, merge((shift = a, name = "Induced MTP (Sum)"), get_effects_tuple(mtp_result_net_a_sum, ψ0, n)))

end

CSV.write(joinpath("results", "mtp_grid_a.csv"), est_grid_a)

```


Let's try the same analysis, but with a multiplicative shift. We can reuse our previously-fitted models and evaluate them on another shift.

```{julia}

mshift = MultiplicativeShift(1.2)

mtp_result_m = getestimate(ModifiedTreatment.estimate(mach, mshift))
mtp_effect_m = get_effects_tuple(mtp_result_m, ψ0, n)

mtp_result_net_m_sum = getestimate(ModifiedTreatment.estimate(mach_net_sum, mshift))
mtp_effect_net_m_sum = get_effects_tuple(mtp_result_net_m_sum, ψ0, n)

ests = [mtp_effect_net_m_sum, mtp_effect_m, glm_effect_m]

tbl = (method = ["Induced MTP (Sum)", "MTP (No Interference)", "GLM (No Interference)"], effects = map(x -> x.est, ests), upper = map(x -> x.upper, ests), lower = map(x -> x.lower, ests))
CSV.write(joinpath("results", "mtp_analysis_m.csv"), tbl)


```

Finally, let's compute the same estimates, except across a grid of possible shifts for the multiplicative shifts as well.
```{julia}

est_grid_m = []
as = 0.04:0.04:0.4
for a in as
        shift = MultiplicativeShift(1.0 + a)
        mtp_result_m = getestimate(estimate(mach, shift))
        mtp_result_net_m_sum = getestimate(estimate(mach_net_sum, shift))

        push!(est_grid_m, merge((shift = a, name = "MTP (No Interference)"), get_effects_tuple(mtp_result_m, ψ0, n)))
        push!(est_grid_m, merge((shift = a, name = "Induced MTP (Sum)"), get_effects_tuple(mtp_result_net_m_sum, ψ0, n)))
end

CSV.write(joinpath("results", "mtp_grid_m.csv"), est_grid_m)

```
